# import requests
# from bs4 import BeautifulSoup
# import pandas as pd

# URL = "https://www.imdb.com/chart/top/"
# HEADERS = {
#     "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
#                   "AppleWebKit/537.36 (KHTML, like Gecko) "
#                   "Chrome/115.0.0.0 Safari/537.36",
#     "Accept-Language": "en-US,en;q=0.9",
# }

# def fetch_page(url: str) -> BeautifulSoup:
#     """Fetches HTML content and returns a BeautifulSoup object."""
#     response = requests.get(url, headers=HEADERS, timeout=10)
#     response.raise_for_status()
#     return BeautifulSoup(response.text, "html.parser")

# def parse_movies(soup: BeautifulSoup) -> list[dict]:
#     """Parses movie data from IMDb Top 250 table."""
#     movies = []
#     table = soup.find("ul", class_="ipc-metadata-list")  # IMDb now uses UL/LI
#     if not table:
#         print("❌ Could not find movie list. Check selectors.")
#         return movies
    
#     items = table.find_all("li", class_="ipc-metadata-list-summary-item")
#     for idx, item in enumerate(items, start=1):
#         title_tag = item.find("h3")
#         rating_tag = item.find("span", class_="ipc-rating-star")
#         year_tag = item.find("span", class_="sc-b189961a-8")
        
#         title = title_tag.text.strip() if title_tag else None
#         rating = rating_tag.text.strip().split()[0] if rating_tag else None
#         year = year_tag.text.strip() if year_tag else None
        
#         movies.append({
#             "Rank": idx,
#             "Title": title,
#             "Year": year,
#             "Rating": rating
#         })
#     return movies

# def save_to_csv(data: list[dict], filename: str) -> None:
#     """Saves extracted data to CSV."""
#     df = pd.DataFrame(data)
#     df.to_csv(filename, index=False, encoding="utf-8")
#     print(f"✅ Saved {len(df)} movies to {filename}")

# def main():
#     print(" Fetching IMDb Top 250...")
#     soup = fetch_page(URL)
#     movies = parse_movies(soup)
#     if movies:
#         save_to_csv(movies, "imdb_top_250.csv")
#     else:
#         print("No movies extracted. Possibly a selector or access issue.")

# if __name__ == "__main__":
#     main()
# # IMDb Top 250 Scraper
# # This script fetches the IMDb Top 250 movies and saves them to a CSV file.

"""
Books to Scrape — Data Extraction Script
----------------------------------------
This script scrapes book data from http://books.toscrape.com
and saves it to a CSV file.

Data collected:
- Title
- Price
- Rating
- Availability

Author: Kayode Micheal Olabode
Date: 2025-08-09

Requirements:
    pip install requests bs4, pandas
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import re

BASE_URL = "http://books.toscrape.com/catalogue/page-{}.html"
HEADERS = {
    "User-Agent": "KayodeBot/1.0 (+https://github.com/yourusername)",
    "Accept-Language": "en-US,en;q=0.9"
}

def fetch_page(url: str) -> BeautifulSoup:
    """Fetch HTML content and return a BeautifulSoup object."""
    response = requests.get(url, headers=HEADERS, timeout=10)
    response.encoding = "utf-8"  # ✅ Force correct encoding
    response.raise_for_status()
    return BeautifulSoup(response.text, "html.parser")

def parse_books(soup: BeautifulSoup) -> list[dict]:
    """Extract book details from a single page."""
    books = []
    for book in soup.find_all("article", class_="product_pod"):
        title = book.h3.a["title"]
        price = book.find("p", class_="price_color").text.strip()
        price = re.sub(r"^[^\£]*", "£", price)  # ✅ Ensure clean £
        rating = book.p["class"][1]  # second class is rating
        availability = book.find("p", class_="instock availability").text.strip()
        
        books.append({
            "Title": title,
            "Price": price,
            "Rating": rating,
            "Availability": availability
        })
    return books

def scrape_all_books() -> list[dict]:
    """Scrape all pages of books and return combined list."""
    all_books = []
    print(" Scraping all pages from BookstoScrape...")
    for page in range(1, 51):  # 50 pages total
        soup = fetch_page(BASE_URL.format(page))
        books = parse_books(soup)
        if not books:
            break
        all_books.extend(books)
    return all_books

def save_to_csv(data: list[dict], filename: str) -> None:
    """Save scraped data to a CSV file with UTF-8 BOM for Excel compatibility."""
    df = pd.DataFrame(data)
    df.to_csv(filename, index=False, encoding="utf-8-sig")  # ✅ Works in Excel
    print(f"✅ Saved {len(df)} books to {filename}")

def main():
    books = scrape_all_books()
    
    # Preview first 100 books in console
    print("\nPreview of scraped books:")
    for book in books[:100]:
        print(f"{book['Title']} | {book['Price']} | {book['Rating']} | {book['Availability']}")
    
    save_to_csv(books, "books.csv")

if __name__ == "__main__":
    main()
# This script scrapes book data from http://books.toscrape.com
# and saves it to a CSV file named "books.csv".